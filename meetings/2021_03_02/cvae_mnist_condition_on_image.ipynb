{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=20\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-conservation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-merchant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-protection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CVAE(nn.Module):\n",
    "#     def __init__(self, x_dim, h_dim, z_dim, y_dim):\n",
    "#         super(CVAE, self).__init__()\n",
    "        \n",
    "#         # encoder part\n",
    "#         self.fc1 = nn.Linear(x_dim + y_dim, h_dim)\n",
    "#         self.fc21 = nn.Linear(h_dim, z_dim)\n",
    "#         self.fc22 = nn.Linear(h_dim, z_dim)\n",
    "#         # decoder part\n",
    "#         self.fc3 = nn.Linear(z_dim + x_dim, h_dim)\n",
    "#         self.fc4 = nn.Linear(h_dim, y_dim)\n",
    "    \n",
    "#     def encoder(self, x, y):\n",
    "#         concat_input = torch.cat([x, y], 1)\n",
    "#         h = F.relu(self.fc1(concat_input))\n",
    "#         return self.fc21(h), self.fc22(h)\n",
    "    \n",
    "#     def sampling(self, mu, log_var):\n",
    "#         std = torch.exp(0.5*log_var)\n",
    "#         eps = torch.randn_like(std)\n",
    "#         return eps.mul(std).add(mu) # return z sample\n",
    "    \n",
    "#     def decoder(self, z, x):\n",
    "#         concat_input = torch.cat([z, x.view(-1, 784)], 1)\n",
    "#         h = F.relu(self.fc3(concat_input))\n",
    "#         return F.log_softmax(self.fc4(h))\n",
    "    \n",
    "#     def forward(self, x, y):\n",
    "#         mu, log_var = self.encoder(x.view(-1, 784), y)\n",
    "#         z = self.sampling(mu, log_var)\n",
    "#         return self.decoder(z, x), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # condition after convolution\n",
    "\n",
    "# class CVAE(nn.Module):\n",
    "#     def __init__(self, x_dim, h1_dim, h2_dim, z_dim, y_dim):\n",
    "#         super(CVAE, self).__init__()\n",
    "        \n",
    "#         # encoder part\n",
    "#         self.fc1 = nn.Linear(x_dim, h1_dim)\n",
    "#         self.fc2 = nn.Linear(h1_dim + y_dim, h2_dim)\n",
    "#         self.fc31 = nn.Linear(h2_dim, z_dim)\n",
    "#         self.fc32 = nn.Linear(h2_dim, z_dim)\n",
    "#         # decoder part\n",
    "#         self.fc4 = nn.Linear(x_dim, h1_dim)\n",
    "#         self.fc5 = nn.Linear(h1_dim + y_dim, h2_dim)\n",
    "#         self.fc6 = nn.Linear(h2_dim, y_dim)\n",
    "    \n",
    "#     def encoder(self, x, y):\n",
    "# #         concat_input = torch.cat([x, y], 1)\n",
    "#         h1 = F.relu(self.fc1(x))\n",
    "#         h2 = F.relu(self.fc2(torch.cat([h1, y], 1)))\n",
    "#         return self.fc31(h2), self.fc32(h2)\n",
    "    \n",
    "#     def sampling(self, mu, log_var):\n",
    "#         std = torch.exp(0.5*log_var)\n",
    "#         eps = torch.randn_like(std)\n",
    "#         return eps.mul(std).add(mu) # return z sample\n",
    "    \n",
    "#     def decoder(self, z, x):\n",
    "#         h1 = F.relu(self.fc4(x.view(-1, 784)))\n",
    "#         h2 = F.relu(self.fc5(torch.cat([h1, z], 1)))\n",
    "#         return F.log_softmax(self.fc6(h2))\n",
    "    \n",
    "#     def forward(self, x, y):\n",
    "#         mu, log_var = self.encoder(x.view(-1, 784), y)\n",
    "#         z = self.sampling(mu, log_var)\n",
    "#         return self.decoder(z, x), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition after convolution\n",
    "# more hidden layers\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, x_dim, hs_dim, z_dim, y_dim, tie_weights=False):\n",
    "        super(CVAE, self).__init__()\n",
    "        \n",
    "        assert len(hs_dim) >= 2\n",
    "        self.tie_weights = tie_weights\n",
    "        \n",
    "        # encoder part for x\n",
    "        self.encode_x = []\n",
    "        for i, h_dim in enumerate(hs_dim[:-1]):\n",
    "            if i == 0:\n",
    "                self.encode_x.append(nn.Linear(x_dim, h_dim))\n",
    "            else:\n",
    "                self.encode_x.append(nn.Linear(hs_dim[i-1], h_dim))\n",
    "        # last layer of encoder combines x and y\n",
    "        self.encode_xy = nn.Linear(hs_dim[-2] + y_dim, hs_dim[-1])\n",
    "        # compute posterior distribution parameters\n",
    "        self.posterior_mean = nn.Linear(hs_dim[-1], z_dim)\n",
    "        self.posterior_logvar = nn.Linear(hs_dim[-1], z_dim)\n",
    "        \n",
    "        if not self.tie_weights:\n",
    "            # decoder part for x\n",
    "            self.decode_x = []\n",
    "            for i, h_dim in enumerate(hs_dim[:-1]):\n",
    "                if i == 0:\n",
    "                    self.decode_x.append(nn.Linear(x_dim, h_dim))\n",
    "                else:\n",
    "                    self.decode_x.append(nn.Linear(hs_dim[i-1], h_dim))\n",
    "        else:\n",
    "            self.decode_x = self.encode_x\n",
    "        # last layer of decoder combines x and z\n",
    "        self.decode_xz = nn.Linear(hs_dim[-2] + z_dim, hs_dim[-1])\n",
    "        # compute y\n",
    "        self.output = nn.Linear(hs_dim[-1], y_dim)\n",
    "    \n",
    "    def encoder(self, x, y):\n",
    "        for layer in self.encode_x:\n",
    "            x = F.relu(layer(x))\n",
    "        h = F.relu(self.encode_xy(torch.cat([x, y], 1)))\n",
    "        return self.posterior_mean(h), self.posterior_logvar(h)\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add(mu) # return z sample\n",
    "    \n",
    "    def decoder(self, z, x):\n",
    "        for layer in self.decode_x:\n",
    "            x = F.relu(layer(x))\n",
    "        h = F.relu(self.decode_xz(torch.cat([x, z], 1)))\n",
    "        return F.log_softmax(self.output(h))\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        x = x.view(-1, 784)\n",
    "        mu, log_var = self.encoder(x, y)\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z, x), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-spiritual",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# z_dim = 10\n",
    "z_dim = 2\n",
    "\n",
    "# cvae = CVAE(x_dim=784, h1_dim=50, h2_dim=10, z_dim=z_dim, y_dim=10)\n",
    "\n",
    "cvae = CVAE(x_dim=784, hs_dim=[20, 10, 10], z_dim=z_dim, y_dim=10)\n",
    "# cvae = CVAE(x_dim=784, hs_dim=[20, 10, 5], z_dim=z_dim, y_dim=10)\n",
    "# cvae = CVAE(x_dim=784, hs_dim=[50, 10, 5], z_dim=z_dim, y_dim=10)\n",
    "# cvae = CVAE(x_dim=784, hs_dim=[50, 10, 10], z_dim=z_dim, y_dim=10)\n",
    "# cvae = CVAE(x_dim=784, hs_dim=[50, 10], z_dim=z_dim, y_dim=10)\n",
    "# cvae = CVAE(x_dim=784, hs_dim=[50, 10, 5, 10], z_dim=z_dim, y_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-saturn",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-canyon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(cvae.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_softmax_loss = nn.NLLLoss(reduction='sum')  \n",
    "\n",
    "# return reconstruction error + KL divergence losses\n",
    "def loss_function(y_pred, y, mu, log_var):\n",
    "    sm_loss = log_softmax_loss(y_pred, y)\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return sm_loss, KLD\n",
    "\n",
    "# one-hot encoding\n",
    "def one_hot(labels, class_size): \n",
    "    targets = torch.zeros(labels.size(0), class_size)\n",
    "    for i, label in enumerate(labels):\n",
    "        targets[i, label] = 1\n",
    "    return Variable(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    cvae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        x_batch = torch.cat([x_batch, x_batch], 0)\n",
    "        \n",
    "        # make toy data\n",
    "        y_batch_p1 = y_batch + 1\n",
    "        y_batch_p1[y_batch_p1==10] = 0\n",
    "        y_batch = torch.cat([y_batch, y_batch_p1], 0)\n",
    "\n",
    "        y_oh_batch = one_hot(y_batch, class_size=10)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred, mu, log_var = cvae(x_batch, y_oh_batch)\n",
    "        sm_loss, KLD = loss_function(y_pred, y_batch, mu, log_var)\n",
    "        loss = sm_loss + KLD\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} = {:.6f} + {:.6f}'.format(\n",
    "                epoch, batch_idx * len(x_batch), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(),\n",
    "            sm_loss.item(), KLD.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-bachelor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    cvae.eval()\n",
    "    test_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            y_oh_batch = one_hot(y_batch, class_size=10)\n",
    "            y_pred, mu, log_var = cvae(x_batch, y_oh_batch)\n",
    "            # sum up batch loss\n",
    "            sm_loss, KLD = loss_function(y_pred, y_batch, mu, log_var)\n",
    "            test_loss += (sm_loss.item() + KLD.item())\n",
    "        \n",
    "    test_loss /= (len(test_loader.dataset)/test_loader.batch_size)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-crazy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-battle",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(4):  # TODO more epochs?\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-tumor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # random image\n",
    "\n",
    "# num_row = 4\n",
    "# num_col = 5\n",
    "# num = num_row * num_col\n",
    "\n",
    "# fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "# for i in range(num):\n",
    "#     ax = axes[i//num_col, i%num_col]\n",
    "#     idx = random.randint(a=0, b=10000)\n",
    "#     tmp = test_dataset[idx][0]\n",
    "#     ax.imshow(tmp.numpy()[0, :, :], cmap='gray', interpolation='none')\n",
    "#     ax.set_title('i:{} l:{}'.format(idx, test_dataset[idx][1]))\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-explosion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-exchange",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-stream",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate prediction\n",
    "# only running 'decoder', i.e. x, z -> y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_debug, y_debug = train_dataset[500]\n",
    "\n",
    "\n",
    "\n",
    "x_debug, y_debug = test_dataset[5]\n",
    "\n",
    "# x_debug, y_debug = test_dataset[7]\n",
    "\n",
    "# x_debug, y_debug = test_dataset[10]\n",
    "\n",
    "# x_debug, y_debug = test_dataset[500]\n",
    "\n",
    "\n",
    "# plot x\n",
    "plt.imshow(x_debug.numpy()[0, :, :], cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "yp_all = []\n",
    "for _ in range(1000):\n",
    "    z_debug = cvae.sampling(torch.Tensor([[0] * z_dim]), torch.Tensor([[0] * z_dim]))  # from prior, z dim = 2\n",
    "    # print(z_debug)\n",
    "    yp_debug = cvae.decoder(x=x_debug.view(-1, 784), z=z_debug)\n",
    "    # print(yp_debug.exp())\n",
    "    yp_all.append(yp_debug.argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_label in range(10):\n",
    "    print(\"Class {}, count {}/{}\".format(class_label, yp_all.count(class_label), len(yp_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-letter",
   "metadata": {},
   "outputs": [],
   "source": [
    "yp_debug.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-cover",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many example predicts bimodal distribution\n",
    "# training\n",
    "\n",
    "n_sample = 100\n",
    "n_datapoint = 1000\n",
    "\n",
    "p_all = []\n",
    "\n",
    "for _ in tqdm(range(n_datapoint)):\n",
    "    idx = random.randint(a=0, b=len(train_dataset)-1)\n",
    "    x_debug, y_debug = train_dataset[idx]\n",
    "\n",
    "    yp_all = []\n",
    "    for _ in range(n_sample):\n",
    "        z_debug = cvae.sampling(torch.Tensor([[0] * z_dim]), torch.Tensor([[0] * z_dim]))  # from prior, z dim = 2\n",
    "        # print(z_debug)\n",
    "        yp_debug = cvae.decoder(x=x_debug.view(-1, 784), z=z_debug)\n",
    "        # print(yp_debug.exp())\n",
    "        yp_all.append(yp_debug.argmax().item())\n",
    "    # calculation percent times the top hit class was predicted\n",
    "    class_count = [yp_all.count(class_label) for class_label in range(10)]\n",
    "    # entropy\n",
    "    p_all.append(entropy(np.asarray(class_count)/n_sample))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(p_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-winter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many example predicts bimodal distribution\n",
    "# testing\n",
    "\n",
    "n_sample = 100\n",
    "n_datapoint = 1000\n",
    "\n",
    "p_all = []\n",
    "\n",
    "for _ in tqdm(range(n_datapoint)):\n",
    "    idx = random.randint(a=0, b=len(test_dataset)-1)\n",
    "    x_debug, y_debug = test_dataset[idx]\n",
    "\n",
    "    yp_all = []\n",
    "    for _ in range(n_sample):\n",
    "        z_debug = cvae.sampling(torch.Tensor([[0] * z_dim]), torch.Tensor([[0] * z_dim]))  # from prior, z dim = 2\n",
    "        # print(z_debug)\n",
    "        yp_debug = cvae.decoder(x=x_debug.view(-1, 784), z=z_debug)\n",
    "        # print(yp_debug.exp())\n",
    "        yp_all.append(yp_debug.argmax().item())\n",
    "    # calculation percent times the top hit class was predicted\n",
    "    class_count = [yp_all.count(class_label) for class_label in range(10)]\n",
    "    # entropy\n",
    "    p_all.append(entropy(np.asarray(class_count)/n_sample))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-posting",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(p_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy([0.5, 0.5, 0, 0, 0, 0, 0, 0, 0, 0])  # this is what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-response",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
