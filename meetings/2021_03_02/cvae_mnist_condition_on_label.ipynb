{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=20\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-conservation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-merchant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-protection",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim, z_dim, y_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim + y_dim, h_dim)\n",
    "        self.fc21 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc22 = nn.Linear(h_dim, z_dim)\n",
    "        # decoder part\n",
    "        self.fc3 = nn.Linear(z_dim + y_dim, h_dim)\n",
    "        self.fc4 = nn.Linear(h_dim, x_dim)\n",
    "    \n",
    "    def encoder(self, x, y):\n",
    "        concat_input = torch.cat([x, y], 1)\n",
    "        h = F.relu(self.fc1(concat_input))\n",
    "        return self.fc21(h), self.fc22(h)\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add(mu) # return z sample\n",
    "    \n",
    "    def decoder(self, z, y):\n",
    "        concat_input = torch.cat([z, y.view(-1, 10)], 1)\n",
    "        h = F.relu(self.fc3(concat_input))\n",
    "        return torch.sigmoid(self.fc4(h))\n",
    "#         return F.log_softmax(self.fc4(h))\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        mu, log_var = self.encoder(x.view(-1, 784), y)\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z, y), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "z_dim = 10\n",
    "\n",
    "cvae = CVAE(x_dim=784, h_dim=200, z_dim=z_dim, y_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(cvae.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_softmax_loss = nn.NLLLoss(reduction='sum')  \n",
    "bce_loss = nn.BCELoss(reduction='sum')\n",
    "\n",
    "# return reconstruction error + KL divergence losses\n",
    "def loss_function(x_pred, x, mu, log_var):\n",
    "#     sm_loss = log_softmax_loss(y_pred, y)\n",
    "    reconstuction_loss = bce_loss(x_pred, x)\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return reconstuction_loss, KLD\n",
    "\n",
    "# one-hot encoding\n",
    "def one_hot(labels, class_size): \n",
    "    targets = torch.zeros(labels.size(0), class_size)\n",
    "    for i, label in enumerate(labels):\n",
    "        targets[i, label] = 1\n",
    "    return Variable(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    cvae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        y_oh_batch = one_hot(y_batch, class_size=10)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x_pred, mu, log_var = cvae(x_batch, y_oh_batch)\n",
    "        reconstuction_loss, KLD = loss_function(x_pred, x_batch, mu, log_var)\n",
    "        loss = reconstuction_loss + KLD\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} = {:.6f} + {:.6f}'.format(\n",
    "                epoch, batch_idx * len(x_batch), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(),\n",
    "            reconstuction_loss.item(), KLD.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-bachelor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    cvae.eval()\n",
    "    test_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            y_oh_batch = one_hot(y_batch, class_size=10)\n",
    "            x_pred, mu, log_var = cvae(x_batch, y_oh_batch)\n",
    "            # sum up batch loss\n",
    "            reconstuction_loss, KLD = loss_function(x_pred, x_batch, mu, log_var)\n",
    "            test_loss += (reconstuction_loss.item() + KLD.item())\n",
    "        \n",
    "    test_loss /= (len(test_loader.dataset)/test_loader.batch_size)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-crazy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-battle",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(4):  # TODO more epochs?\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-major",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-spread",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_row = 5   # number of samples per class\n",
    "num_col = 10  # each column is one class\n",
    "num = num_row * num_col\n",
    "\n",
    "fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "\n",
    "for idx_col in range(num_col):\n",
    "    y_debug = one_hot(torch.from_numpy(np.asarray([idx_col])), 10) # class label = idx_col\n",
    "    for idx_row in range(num_row):\n",
    "        ax = axes[idx_row, idx_col]\n",
    "        # sample from prior\n",
    "        z_debug = cvae.sampling(torch.Tensor([[0] * z_dim]), torch.Tensor([[0] * z_dim]))\n",
    "        xp_debug = cvae.decoder(z=z_debug, y=y_debug)\n",
    "        ax.imshow(xp_debug.detach().numpy().reshape(28, 28), cmap='gray', interpolation='none')\n",
    "#         ax.set_title('i:{} l:{}'.format(idx, test_dataset[idx][1]))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "        \n",
    "\n",
    "# for i in range(num):\n",
    "#     idx_row = i//num_col\n",
    "#     idx_col = i%num_col\n",
    "#     ax = axes[idx_row, idx_col]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     idx = random.randint(a=0, b=10000)\n",
    "#     tmp = test_dataset[idx][0]\n",
    "#     ax.imshow(tmp.numpy()[0, :, :], cmap='gray', interpolation='none')\n",
    "#     ax.set_title('i:{} l:{}'.format(idx, test_dataset[idx][1]))\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-response",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
